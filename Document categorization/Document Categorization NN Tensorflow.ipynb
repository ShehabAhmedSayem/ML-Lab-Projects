{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zayd/.local/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102099\n"
     ]
    }
   ],
   "source": [
    "# Dataset loading\n",
    "\n",
    "news_test = datasets.load_files('./category', encoding='utf-8')\n",
    "x_train, x_test, y_train, y_test = train_test_split(news_test.data, news_test.target, test_size=0.2)\n",
    "\n",
    "print(len(news_test.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_2_index(vocab):\n",
    "    word2index = {}\n",
    "    for i,word in enumerate(vocab):\n",
    "        word2index[word] = i\n",
    "\n",
    "    return word2index\n",
    "\n",
    "\n",
    "def get_batch(df_train, df_test,i,batch_size):\n",
    "    batches = []\n",
    "    results = []\n",
    "    texts = df_train[i*batch_size:i*batch_size+batch_size]\n",
    "    categories = df_test[i*batch_size:i*batch_size+batch_size]\n",
    "    \n",
    "    for text in texts:\n",
    "        layer = np.zeros(total_words,dtype=float)\n",
    "        for word in text.split(' '):\n",
    "            layer[word2index[word]] += 1\n",
    "\n",
    "        batches.append(layer)\n",
    "\n",
    "    for category in categories:\n",
    "        y = np.zeros((12),dtype=float)\n",
    "\n",
    "        for i in range(12):\n",
    "            if category == i:\n",
    "                y[i] = 1.\n",
    "                \n",
    "        results.append(y)\n",
    "\n",
    "    return np.array(batches),np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "865455\n"
     ]
    }
   ],
   "source": [
    "vocab = Counter()\n",
    "\n",
    "for text in x_train:\n",
    "    for word in text.split(' '):\n",
    "        vocab[word]+=1\n",
    "\n",
    "for text in x_test:\n",
    "    for word in text.split(' '):\n",
    "        vocab[word]+=1\n",
    "\n",
    "total_words = len(vocab)\n",
    "\n",
    "print(total_words)\n",
    "\n",
    "word2index = get_word_2_index(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:From <ipython-input-5-d78ab191a736>:48: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Neural network model \n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "epochs = 10\n",
    "batch_size = 150\n",
    "display_step = 1\n",
    "\n",
    "input_num = total_words\n",
    "output_num = 12\n",
    "\n",
    "layer_1_nodes = 50\n",
    "layer_2_nodes = 100\n",
    "layer_3_nodes = 50\n",
    "\n",
    "with tf.variable_scope('input'):\n",
    "    X = tf.placeholder(tf.float32, shape=(None, input_num))\n",
    "    \n",
    "# Layer 1\n",
    "with tf.variable_scope('layer_1'):\n",
    "    weights = tf.get_variable(\"weights1\", shape=[input_num, layer_1_nodes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases1\", shape=[layer_1_nodes], initializer=tf.zeros_initializer())\n",
    "    layer_1_output = tf.nn.relu(tf.matmul(X, weights) + biases)\n",
    "\n",
    "# Layer 2\n",
    "with tf.variable_scope('layer_2'):\n",
    "    weights = tf.get_variable(\"weights2\", shape=[layer_1_nodes, layer_2_nodes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases2\", shape=[layer_2_nodes], initializer=tf.zeros_initializer())\n",
    "    layer_2_output = tf.nn.relu(tf.matmul(layer_1_output, weights) + biases)\n",
    "\n",
    "# Layer 3\n",
    "with tf.variable_scope('layer_3'):\n",
    "    weights = tf.get_variable(\"weights3\", shape=[layer_2_nodes, layer_3_nodes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases3\", shape=[layer_3_nodes], initializer=tf.zeros_initializer())\n",
    "    layer_3_output = tf.nn.relu(tf.matmul(layer_2_output, weights) + biases)\n",
    "\n",
    "# Output Layer\n",
    "with tf.variable_scope('output'):\n",
    "    weights = tf.get_variable(\"weights4\", shape=[layer_3_nodes, output_num], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases4\", shape=[output_num], initializer=tf.zeros_initializer())\n",
    "    prediction = tf.matmul(layer_3_output, weights) + biases\n",
    "\n",
    "    \n",
    "# The cost function of the neural network\n",
    "\n",
    "with tf.variable_scope('cost'):\n",
    "    Y = tf.placeholder(tf.float32, shape=(None, output_num))\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=prediction, labels=Y))\n",
    "\n",
    "# The optimizer function\n",
    "\n",
    "with tf.variable_scope('train'):\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Epoch: 0001 loss= 0.815284426\n",
      "Epoch: 0002 loss= 0.388721734\n",
      "Epoch: 0003 loss= 0.316627504\n",
      "Epoch: 0004 loss= 0.287953078\n",
      "Epoch: 0005 loss= 0.277781004\n"
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Training cycle\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(len(x_train)/batch_size)\n",
    "\n",
    "        # Loop over all batches\n",
    "\n",
    "        for i in range(total_batch):\n",
    "            batch_x,batch_y = get_batch(x_train,y_train,i,batch_size)\n",
    "\n",
    "            c,_ = sess.run([cost,optimizer], feed_dict={X: batch_x,Y:batch_y})\n",
    "            avg_cost += c / total_batch\n",
    "            \n",
    "\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"loss=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    \n",
    "\n",
    "        '''# Test model\n",
    "        correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "        total_test_data = len(x_test)\n",
    "        batch_x_test,batch_y_test = get_batch(x_test,y_test,0,total_test_data)\n",
    "        print(\"Accuracy:\", accuracy.eval({input_tensor: batch_x_test, output_tensor: batch_y_test}))'''\n",
    "        \n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda3]",
   "language": "python",
   "name": "conda-env-anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
